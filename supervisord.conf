[supervisord]
nodaemon=true
logfile=/dev/stdout
logfile_maxbytes=0
; optional:
; pidfile=/tmp/supervisord.pid
; loglevel=info

[program:ollama]
command=ollama serve
autorestart=true
stdout_logfile=/dev/stdout
stderr_logfile=/dev/stderr
environment=OLLAMA_URL="http://127.0.0.1:11434"
stdout_logfile_maxbytes=0
stderr_logfile_maxbytes=0
priority=10

[program:pull-model]
command=sh -lc 'if [ -n "$OLLAMA_MODEL" ]; then \
  for i in $(seq 1 60); do curl -fsS http://127.0.0.1:11434/api/version && break || sleep 1; done; \
  echo "Pulling model: $OLLAMA_MODEL" && ollama pull "$OLLAMA_MODEL"; \
  else echo "No OLLAMA_MODEL set; skipping pull."; fi'
autorestart=false
startretries=0
stdout_logfile=/dev/stdout
stderr_logfile=/dev/stderr
stdout_logfile_maxbytes=0
stderr_logfile_maxbytes=0
priority=20
startsecs=0

[program:mcp-server]
command=python -m kimai.kimai
autorestart=true
stdout_logfile=/dev/stdout
stderr_logfile=/dev/stderr
stdout_logfile_maxbytes=0
stderr_logfile_maxbytes=0
environment=PYTHONUNBUFFERED="1"
priority=30

[program:bridge]
; Expose on 11435; set OLLAMA_URL to your Ollama (inside or outside container)
command=sh -lc 'for i in $(seq 1 60); do curl -fsS http://127.0.0.1:11434/api/version && break || sleep 1; done; \
  exec ollama-mcp-bridge \
    --host 0.0.0.0 \
    --port 11435 \
    --config /app/mcp-config.json \
    --ollama-url http://127.0.0.1:11434'
autorestart=true
stdout_logfile=/dev/stdout
stderr_logfile=/dev/stderr
stdout_logfile_maxbytes=0
stderr_logfile_maxbytes=0
priority=40

[program:ui-proxy]
command=uvicorn proxy:app --host 0.0.0.0 --port 8080
directory=/app
autorestart=true
stdout_logfile=/dev/stdout
stderr_logfile=/dev/stderr
stdout_logfile_maxbytes=0
stderr_logfile_maxbytes=0
priority=50
environment=BRIDGE_URL="http://127.0.0.1:11435"