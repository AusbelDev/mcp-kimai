services:
  mcp-ollama:
    image: mcp-ollama-chat
    container_name: mcp-ollama
    ports:
      - "11434:11434" # Ollama (optional to expose)
      - "11435:11435" # Bridge (Open WebUI will hit this)
    environment:
      # Pre-pull one model at boot (optional)
      OLLAMA_MODEL: qwen3:1.7b
    volumes:
      - ./mcp_context:/app/mcp_context:rw

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    depends_on:
      - mcp-ollama
    ports:
      - "3000:8080" # Open WebUI at http://localhost:3000
    environment:
      # Point Open WebUI to the *bridge* (Ollama-compatible + MCP)
      OLLAMA_BASE_URL: "http://mcp-ollama:11435"
    volumes:
      - openwebui_data:/app/backend/data
    restart: unless-stopped

volumes:
  openwebui_data:
